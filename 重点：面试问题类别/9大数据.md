## 问题大数据相关 流程熟悉去学习
1.spark输出文件的个数，如何合并小文件

12.hive的优化
13.讲讲hadoop和mapreduce
14.讲讲spark




1.mapreduce重构成spark任务的限制
2.mapreduce和spark的区别
3.hive的优化
4.spark提交job流程


6.hive中udf实际用过吗。具体处理流程

9.spark为什么用scala，scala和java的不同



spark中有partition的概念，每个partition都会对应一个task，task越多，在处理大规模数据的时候，就会越有效率。不过task并不是越多越好，如果平时测试，或者数据量没有那么大，则没有必要task数量太多。

简单介绍 MapReduce 原理，有没有看过源码，说说 Map 阶段怎么实现的,
MapReduce 实现统计出现次数最多的前 100 个访问 IP.
MapReduce 实现统计不重复用户 ID,MapReduce 实现两个数据集求交集。
HBase 行健怎么设计,spark 性能一般优化方法,spark streaming 和 storm 区别.给了一张笔试题， 10 道选择，一道大题。选择题是 java 基础知识，大题一个有三问：根据场景写出 Hive 建表语句； Hsql 从表中查询；
用MapReduce写好友推荐，在一堆单词里面找出现次数最多的k个
用分布式的方法做采样怎么保证采样结果完全符合预期？
后面又问了Hadoop,Spark,storm下面的产品，原理，适用场景，
写一个 Hadoop 版本的 wordcount。