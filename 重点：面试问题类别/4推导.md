1经典机器学习算法，至少要能亲手推到出来，如LR，SVM，朴素贝叶斯，xgboost，cbow等等，我提到的这几个算法可各个不简单，尤其是LR和SVM，真别看它们简单，也真的不要轻易说自己很懂，不然有可能会被问得很惨（用到上述算法的，那就一定要弄到非常清楚

3.挑一种激活函数推导梯度下降的过程。（知识+逻辑）
FM前向过程

5、lr为什么用sigmoid函数作为概率函数。我：lr是基于伯努利分布为假设的，伯努利分布的指数形式就是sigmoid函数，而且sigmoid函数可以将数据压缩到0-1内，以便表示概率（sif优点）

基于模型（MF
推导LR过程


那推导下SVM吧。这个我会，推出来了，但是到对偶条件这里，面试官问为什么能用对偶条件，我没答上来，还是太菜。
那再推下lr吧。这次顺利的推了出来，面试官问的问题也回答了上来。顺利通过了。
8、xgb的loss函数的推导（mse以及非mse形式），以及求解推导。
9、xgb是如何实现并行的。正则化怎么实现（重要
分类树和回归树的区别是什么？（知识决策树节点分裂时是如何选择特征的


2. 算法原理与公式推导，高频题：lr，svm推导；xgboost，gbdt，lightgbm；随机森林，决策树；lstm公式和反向传播等
3. 概率题数学题备战
4. 大数据题备战

## 腾讯
1.清洗数据用的什么特殊的方法？为什么这么做？ 
2.逻辑回归里面是线性可分的，怎么做才能让它变成非线性可分？ 

3.SGD你觉得在使用的过程中有什么缺点？ 
4.为什么传统的机器学习方法在解决不平衡问题的时候会出现问题？ 
5.树模型（非线性模型）和线性模型的区别？树模型的优势是什么？ 
6.随机森林还是哪个模型忘了怎么才能学习出特征和特征之间的关系？ 
二面：
1.比赛：介绍天池的口碑比赛，跟前面的队伍差距在哪里？ 
2.逻辑回归里面SGD有时候步长设置的不对，收敛会出现问题，该怎么解决？（减小学习率或者增大Batch Size） 
3.如果在逻辑回归模型里面有时一直收敛不到阈值以下的参数解，该怎么做？ 
4.逻辑回归里L1和L2正则化的区别？ 
5.一些优化方法的区别？（BGD、SGD等） 
6.问我深度学习Deep Learning的一些知识？（不会） 
结果：
二面挂。逻辑回归的东西被问得还是很多的，也许是他们平时在业务里面就是用的逻辑回归吧，反正LR这种经典的模型肯定要精通的。


2.手推LR
1.写sigmoid, softmax函数 

.用代码写出Logistic regression的损失函数，并提问了这个损失函数如何推导出的。 
3.介绍SVM 

3.问了SVM的原理，但没有问的很细
2.讲LR，面试官要求LR讲得特别仔细，就那种LR是什么假设，损失函数是怎么回事，怎样更新参数什么的


2.用代码写出Logistic regression的损失函数，并提问了这个损失函数如何推导出的。 
3.介绍SVM 

3.问了SVM的原理，但没有问的很细
2.讲LR，面试官要求LR讲得特别仔细，就那种LR是什么假设，损失函数是怎么回事，怎样更新参数什么的

2.手推LR、手推最小二乘
1.手推SVM
手推前向传播
手写softmax，手写BN公式。
手写协同过滤，FM

## 看比赛整理
，面试官比较关注特征工程。详细问了特征如何设计、为什么这么设计、那些特征会比较重要。商品推荐中如何进行个性化推荐，特征你会想到那些，为什么这么做

。商品推荐中如何进行个性化推荐，特征你会想到那些，为什么这么做：
看比赛
匹配特征，基于用户的统计特征，


svm了解吗，什么样的函数能做核函数



